{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!cd ./automl/efficientdet; pip install -r requirements.txt"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI (from -r requirements.txt (line 14))\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-2l4lxfag\n",
      "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-2l4lxfag\n",
      "Requirement already satisfied: lxml>=4.6.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.6.3)\n",
      "Requirement already satisfied: absl-py>=0.10.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/plass-heesu/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.4 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
      "Requirement already satisfied: Pillow>=6.0.0 in /home/plass-heesu/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (8.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/plass-heesu/.local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (5.4.1)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.15.0)\n",
      "Requirement already satisfied: tensorflow>=2.4.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-addons>=0.12 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (0.13.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.11 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (0.12.0)\n",
      "Requirement already satisfied: neural-structured-learning>=1.3.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.5 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.6.0)\n",
      "Requirement already satisfied: Cython>=0.29.13 in /home/plass-heesu/.local/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (0.29.24)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from pycocotools==2.0->-r requirements.txt (line 14)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/plass-heesu/.local/lib/python3.7/site-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/plass-heesu/.local/lib/python3.7/site-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/plass-heesu/.local/lib/python3.7/site-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/plass-heesu/.local/lib/python3.7/site-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.12.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.17.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.3.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.5.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.36.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.32.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow-addons>=0.12->-r requirements.txt (line 9)) (2.12.1)\n",
      "Requirement already satisfied: scipy in /home/plass-heesu/.local/lib/python3.7/site-packages (from neural-structured-learning>=1.3.1->-r requirements.txt (line 11)) (1.7.0)\n",
      "Requirement already satisfied: attrs in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from neural-structured-learning>=1.3.1->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.5->-r requirements.txt (line 12)) (0.1.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.6.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.5.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "!pip uninstall -y numpy\n",
    "!pip install numpy==1.20"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: numpy 1.19.5\n",
      "Uninstalling numpy-1.19.5:\n",
      "  Successfully uninstalled numpy-1.19.5\n",
      "Collecting numpy==1.20\n",
      "  Using cached numpy-1.20.0-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.20.0 which is incompatible.\n",
      "tensorflow-gpu 2.4.0 requires numpy~=1.19.2, but you have numpy 1.20.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.20.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "!conda list numpy\n",
    "!conda list tensorflow\n",
    "!conda list python"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# packages in environment at /home/plass-heesu/anaconda3/envs/munsutf2.4:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "numpy                     1.20.0                   pypi_0    pypi\n",
      "# packages in environment at /home/plass-heesu/anaconda3/envs/munsutf2.4:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "tensorflow                2.4.1           mkl_py37h2d14ff2_0  \n",
      "tensorflow-addons         0.13.0                   pypi_0    pypi\n",
      "tensorflow-base           2.4.1           mkl_py37h43e0292_0  \n",
      "tensorflow-estimator      2.4.0                    pypi_0    pypi\n",
      "tensorflow-gpu            2.4.0                    pypi_0    pypi\n",
      "tensorflow-hub            0.12.0                   pypi_0    pypi\n",
      "tensorflow-model-optimization 0.6.0                    pypi_0    pypi\n",
      "# packages in environment at /home/plass-heesu/anaconda3/envs/munsutf2.4:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "ipython                   7.22.0           py37hb070fc8_0  \n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
      "opencv-python             4.5.3.56                 pypi_0    pypi\n",
      "python                    3.7.10               h12debd9_4  \n",
      "python-dateutil           2.8.2              pyhd3eb1b0_0  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from glob import glob\n",
    "\n",
    "dataset_dir=os.path.join(os.getcwd(),'Data') #데이터 셋까지의 경로\n",
    "print(dataset_dir)\n",
    "class_list=os.listdir(dataset_dir) #클래스를 전부 class_list에 입력\n",
    "print(class_list)\n",
    "print(len(class_list))\n",
    "\n",
    "train_dir=os.path.join(dataset_dir,'train')\n",
    "test_dir=os.path.join(dataset_dir,'test')\n",
    "print(os.listdir(train_dir))\n",
    "print(os.listdir(test_dir))\n",
    "\n",
    "train_png_dir=os.path.join(train_dir,'PngImages')\n",
    "train_xml_dir=os.path.join(train_dir,'XmlFiles')\n",
    "test_png_dir=os.path.join(test_dir,'PngImages')\n",
    "test_xml_dir=os.path.join(test_dir,'XmlFiles')\n",
    "print(train_png_dir)\n",
    "\n",
    "train_png_path=glob(os.path.join(train_png_dir,'*.png'))\n",
    "train_xml_path=glob(os.path.join(train_xml_dir,'*.xml'))\n",
    "test_png_path=glob(os.path.join(test_png_dir,'*.png'))\n",
    "test_xml_path=glob(os.path.join(test_xml_dir,'*.xml'))\n",
    "print(train_png_path[0])\n",
    "print(train_xml_path[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/sda1/KimHeeSu/munsu/Hanssem/Data\n",
      "['test', 'train']\n",
      "2\n",
      "['PngImages', 'XmlFiles']\n",
      "['PngImages', 'XmlFiles']\n",
      "/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages\n",
      "/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/100356.png\n",
      "/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/100356.xml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "file_id_list=sorted([xml_file[:xml_file.rfind('.xml')] for xml_file in os.listdir(train_xml_dir) \\\n",
    "    if '.xml' in xml_file])\n",
    "#print(file_id_list)\n",
    "len(file_id_list)\n",
    "\n",
    "#하지만 예제에서는 dataframe을 사용했기 때문에 동일하게 하기 위해 df생성\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "train_df=pd.DataFrame({'file_id':file_id_list}) #이전전 셀에서 train_xml_dir를 사용해서  그대로 사용\n",
    "train_df['image_path']=train_png_dir+\"/\"+train_df['file_id']+'.png'\n",
    "train_df['xml_path']=train_xml_dir+\"/\"+train_df['file_id']+'.xml'\n",
    "\n",
    "print(train_df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3123, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  file_id                                                        image_path  \\\n",
       "0  100356  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/100356.png   \n",
       "1  100357  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/100357.png   \n",
       "2  102460  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/102460.png   \n",
       "3  102461  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/102461.png   \n",
       "4  102463  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/102463.png   \n",
       "\n",
       "                                                          xml_path  \n",
       "0  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/100356.xml  \n",
       "1  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/100357.xml  \n",
       "2  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/102460.xml  \n",
       "3  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/102461.xml  \n",
       "4  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/102463.xml  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>xml_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100356</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/100356.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/100356.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100357</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/100357.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/100357.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102460</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/102460.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/102460.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102461</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/102461.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/102461.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102463</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/PngImages/102463.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/train/XmlFiles/102463.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "file_id_list=sorted([xml_file[:xml_file.rfind('.xml')] for xml_file in os.listdir(test_xml_dir) \\\n",
    "    if '.xml' in xml_file])\n",
    "#print(file_id_list)\n",
    "len(file_id_list)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "test_df=pd.DataFrame({'file_id':file_id_list})\n",
    "test_df['image_path']=test_png_dir+\"/\"+test_df['file_id']+'.png'\n",
    "test_df['xml_path']=test_xml_dir+\"/\"+test_df['file_id']+'.xml'\n",
    "\n",
    "print(test_df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(761, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  file_id                                                       image_path  \\\n",
       "0  102458  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102458.png   \n",
       "1  102460  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102460.png   \n",
       "2  102471  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102471.png   \n",
       "3  102477  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102477.png   \n",
       "4  103933  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/103933.png   \n",
       "\n",
       "                                                         xml_path  \n",
       "0  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102458.xml  \n",
       "1  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102460.xml  \n",
       "2  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102471.xml  \n",
       "3  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102477.xml  \n",
       "4  /mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/103933.xml  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>xml_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102458</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102458.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102458.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102460</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102460.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102460.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102471</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102471.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102471.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102477</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/102477.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/102477.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103933</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/PngImages/103933.png</td>\n",
       "      <td>/mnt/sda1/KimHeeSu/munsu/Hanssem/Data/test/XmlFiles/103933.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#epochs 시마다 학습된 weight 파일을 저장한 디렉토리 Google drive로 설정\n",
    "#Google Drive 접근을 위한 Mount 적용\n",
    "#이지만 weight저장할 디렉토리는 /Hanssem/EfficientDet_Weight파일\n",
    "\n",
    "import os, sys\n",
    "\n",
    "model_weight=os.path.join(os.getcwd(),'EfficientDet_Weight')\n",
    "print(model_weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/sda1/KimHeeSu/munsu/Hanssem/EfficientDet_Weight\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "MODEL='efficientdet-d2'\n",
    "\n",
    "def download(m):\n",
    "    if m not in os.listdir(model_weight):\n",
    "        !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{m}.tar.gz\n",
    "        !tar zxf {m}.tar.gz\n",
    "    ckpt_path=os.path.join(model_weight, m)\n",
    "    return ckpt_path\n",
    "\n",
    "#Download checkpoint\n",
    "ckpt_path=download(MODEL)\n",
    "print('Use model in {}'.format(ckpt_path))\n",
    "#모델?이 다운받기만 되고 압축 풀기는 안되서 그냥 파일질라로 로컬로 가져와 압출 풀고 model_weight로 설정한 주소 안에 efficientdet-d2파일 만들어서 그 안에 넣어줌"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Use model in /mnt/sda1/KimHeeSu/munsu/Hanssem/EfficientDet_Weight/efficientdet-d2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#train_file_pattern, test_file_pattern에 사용하기 위한 주소\n",
    "train_temp=os.path.join(os.getcwd(),'tfrecord','train')\n",
    "test_temp=os.path.join(os.getcwd(),'tfrecord','test')\n",
    "print(train_temp+'/*.tfrecord')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/sda1/KimHeeSu/munsu/Hanssem/tfrecord/train/*.tfrecord\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class TRAIN_CFG:\n",
    "    model_name='efficientdet-d2'\n",
    "    strategy=''\n",
    "    model_dir=model_weight\n",
    "    pretrained_ckpt = ckpt_path#coco로 pretrained된 checkpoint파일이 있는 디렉토리 위치\n",
    "\n",
    "    #num_classes=3으로 변경\n",
    "    hparams='num_classes=3,moving_average_decay=0,mixed_precision=true'\n",
    "    use_xla=False\n",
    "    use_fake_data=False\n",
    "\n",
    "    #max_instances_per_image를 200으로 설정하고\n",
    "    #d2모델이 상대적으로 더 크기 때문에 batch_size를 8로 설정시 GPU Out of Memory발생\n",
    "    batch_size=4\n",
    "    eval_samples=5000 #evaluation image데이터 갯수\n",
    "    steps_per_execution=1 #train시 steps 횟수\n",
    "    num_examples_per_epoch=3000 #1epochs 시 적용하는 examples 개수\n",
    "    num_epochs=20 #epochs 횟수\n",
    "    train_file_pattern=train_temp+'/*.tfrecord'\n",
    "    test_file_pattern=test_temp+'/*.tfrecord'\n",
    "    test_json_file=None #optional coco validation json\n",
    "    mode='traineval' #train만 적용 또는 train과 eval함께 적용(traineval)\n",
    "    num_shards=100\n",
    "    max_instances_per_image=100\n",
    "\n",
    "    num_cores=2\n",
    "    tpu=None\n",
    "    gcp_project=None\n",
    "    tpu_zone=None\n",
    "    eval_master=''\n",
    "    eval_name=None\n",
    "    tf_random_seed=2021\n",
    "    profile=False\n",
    "    debug=False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!conda list numpy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# packages in environment at /home/plass-heesu/anaconda3/envs/munsutf2.4:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "numpy                     1.20.0                   pypi_0    pypi\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tf.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sys.path.append('./automl/efficientdet')\n",
    "\n",
    "from keras.train import setup_model\n",
    "import hparams_config\n",
    "\n",
    "import utils\n",
    "from keras import tfmot\n",
    "from keras import train_lib\n",
    "from keras import util_keras\n",
    "\n",
    "config=hparams_config.get_detection_config(TRAIN_CFG.model_name)\n",
    "config.override(TRAIN_CFG.hparams)\n",
    "\n",
    "steps_per_epoch=TRAIN_CFG.num_examples_per_epoch // TRAIN_CFG.batch_size\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    ds_strategy=tf.distribute.OneDeviceStrategy('device:GPU:0')\n",
    "else:\n",
    "    ds_strategy=tf.distribute.OneDeviceStrategy('device:CPU:0')\n",
    "\n",
    "print(ds_strategy)\n",
    "\n",
    "\n",
    "params=dict(\n",
    "    profile=TRAIN_CFG.profile,\n",
    "    mode=TRAIN_CFG.mode,\n",
    "    model_name=TRAIN_CFG.model_name,\n",
    "    steps_per_execution=TRAIN_CFG.steps_per_execution,\n",
    "    num_epochs=TRAIN_CFG.num_epochs,\n",
    "    model_dir=TRAIN_CFG.model_dir,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    strategy=TRAIN_CFG.strategy,\n",
    "    batch_size=TRAIN_CFG.batch_size,\n",
    "    tf_random_seed=TRAIN_CFG.tf_random_seed,\n",
    "    debug=TRAIN_CFG.debug,\n",
    "    test_json_file=TRAIN_CFG.test_json_file,\n",
    "    eval_samples=TRAIN_CFG.eval_samples,\n",
    "    num_shards=ds_strategy.num_replicas_in_sync,\n",
    "    max_instances_per_image = TRAIN_CFG.max_instances_per_image\n",
    ")\n",
    "\n",
    "config.override(params, True)\n",
    "\n",
    "#image size를 tuple 형태로 변환. 512는 (512,512)로 '1920x880'은 (1920, 880)으로 변환\n",
    "config.image_size=utils.parse_image_size(config.image_size)\n",
    "print(config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategyV1 object at 0x7f6da833d9d0>\n",
      "act_type: swish\n",
      "alpha: 0.25\n",
      "anchor_scale: 4.0\n",
      "apply_bn_for_resampling: true\n",
      "aspect_ratios:\n",
      "- 1.0\n",
      "- 2.0\n",
      "- 0.5\n",
      "autoaugment_policy: null\n",
      "backbone_config: null\n",
      "backbone_name: efficientnet-b2\n",
      "batch_size: 4\n",
      "box_class_repeats: 3\n",
      "box_loss_weight: 50.0\n",
      "ckpt_var_scope: null\n",
      "clip_gradients_norm: 10.0\n",
      "conv_after_downsample: false\n",
      "conv_bn_act_pattern: false\n",
      "data_format: channels_last\n",
      "dataset_type: null\n",
      "debug: false\n",
      "delta: 0.1\n",
      "drop_remainder: true\n",
      "eval_samples: 5000\n",
      "first_lr_drop_epoch: 200.0\n",
      "fpn_cell_repeats: 5\n",
      "fpn_config: null\n",
      "fpn_name: null\n",
      "fpn_num_filters: 112\n",
      "fpn_weight_method: null\n",
      "gamma: 1.5\n",
      "grad_checkpoint: false\n",
      "grid_mask: false\n",
      "heads:\n",
      "- object_detection\n",
      "image_size: !!python/tuple\n",
      "- 768\n",
      "- 768\n",
      "img_summary_steps: null\n",
      "input_rand_hflip: true\n",
      "iou_loss_type: null\n",
      "iou_loss_weight: 1.0\n",
      "is_training_bn: true\n",
      "jitter_max: 2.0\n",
      "jitter_min: 0.1\n",
      "label_map: null\n",
      "label_smoothing: 0.0\n",
      "learning_rate: 0.08\n",
      "loss_scale: null\n",
      "lr_decay_method: cosine\n",
      "lr_warmup_epoch: 1.0\n",
      "lr_warmup_init: 0.008\n",
      "map_freq: 5\n",
      "max_instances_per_image: 100\n",
      "max_level: 7\n",
      "mean_rgb:\n",
      "- 123.675\n",
      "- 116.28\n",
      "- 103.53\n",
      "min_level: 3\n",
      "mixed_precision: true\n",
      "mode: traineval\n",
      "model_dir: /mnt/sda1/KimHeeSu/munsu/Hanssem/EfficientDet_Weight\n",
      "model_name: efficientdet-d2\n",
      "model_optimizations: {}\n",
      "momentum: 0.9\n",
      "moving_average_decay: 0\n",
      "name: efficientdet-d2\n",
      "nms_configs:\n",
      "    iou_thresh: null\n",
      "    max_nms_inputs: 0\n",
      "    max_output_size: 100\n",
      "    method: gaussian\n",
      "    pyfunc: false\n",
      "    score_thresh: 0.0\n",
      "    sigma: null\n",
      "num_classes: 3\n",
      "num_epochs: 20\n",
      "num_scales: 3\n",
      "num_shards: 1\n",
      "optimizer: sgd\n",
      "poly_lr_power: 0.9\n",
      "positives_momentum: null\n",
      "profile: false\n",
      "regenerate_source_id: false\n",
      "sample_image: null\n",
      "save_freq: epoch\n",
      "scale_range: false\n",
      "second_lr_drop_epoch: 250.0\n",
      "seg_num_classes: 3\n",
      "separable_conv: true\n",
      "skip_crowd_during_training: true\n",
      "skip_mismatch: true\n",
      "stddev_rgb:\n",
      "- 58.395\n",
      "- 57.120000000000005\n",
      "- 57.375\n",
      "steps_per_epoch: 750\n",
      "steps_per_execution: 1\n",
      "strategy: ''\n",
      "survival_prob: null\n",
      "target_size: null\n",
      "test_json_file: null\n",
      "tf_random_seed: 2021\n",
      "tflite_max_detections: 100\n",
      "use_keras_model: true\n",
      "var_freeze_expr: null\n",
      "verbose: 1\n",
      "weight_decay: 4.0e-05\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#numpy를 1.19로 설정 후 코드를 다시 돌리자 위의 셀에서 오류 발생\n",
    "#ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject\n",
    "#해당 오류로 인해 EfficientDet_tfrecord.ipynb에서 numpy버전을 1.21로 다시 깔았던 것을 생각하면 numpy버전은 1.21로 해야되는 듯 \n",
    "#numpy가 1.19.5일때는 해당 셀 돌아감(가장 위의 requirments.txt실행 시?) 아래에 tf.keras.backend.clear_session() 나오는 셀이 안돌아감\n",
    "#NotImplementedError: Cannot convert a symbolic Tensor (parser/strided_slice_16:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
    "#오류 발생"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import utils\n",
    "from keras import tfmot\n",
    "from keras import train_lib\n",
    "from keras import util_keras\n",
    "#p100에서는 적용할 필요 없음\n",
    "\n",
    "'''\n",
    "precision = utils.get_precision(config.strategy, config.mixed_precision)\n",
    "policy = tf.keras.mixed_precision.Policy(precision)\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nprecision = utils.get_precision(config.strategy, config.mixed_precision)\\npolicy = tf.keras.mixed_precision.Policy(precision)\\ntf.keras.mixed_precision.set_global_policy(policy)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "import dataloader\n",
    "\n",
    "def get_dataset(is_training, config):\n",
    "  # is_training이 True이면 TRAIN_CFG의 train_file_pattern, 그렇지 아니면 val_file_pattern\n",
    "  file_pattern = (\n",
    "    TRAIN_CFG.train_file_pattern\n",
    "    if is_training else TRAIN_CFG.test_file_pattern)\n",
    "  if not file_pattern:\n",
    "    raise ValueError('No matching files.')\n",
    "\n",
    "  return dataloader.InputReader(\n",
    "    file_pattern,\n",
    "    is_training=is_training,\n",
    "    use_fake_data=TRAIN_CFG.use_fake_data,\n",
    "    max_instances_per_image=config.max_instances_per_image,\n",
    "    debug=TRAIN_CFG.debug)(\n",
    "        config.as_dict())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from keras import train_lib\n",
    "from keras import train\n",
    "\n",
    "def get_efficientdet_model(config):\n",
    "    #3개의 class를 가진 efficientdet d2모델을 생성\n",
    "    model=train_lib.EfficientDetNetTrain(config=config)\n",
    "    model=train.setup_model(model, config)\n",
    "    #만약 pretrained 모델이 있으면, 해당 checkpoing weight를 모델로 로딩\n",
    "    #이때 classification layer는 제외\n",
    "    #class TRAIN_CFG : pretrained_ckpt='/content/efficientdet-d2\n",
    "    if TRAIN_CFG.pretrained_ckpt:\n",
    "        ckpt_path=tf.train.latest_checkpoint(TRAIN_CFG.pretrained_ckpt)\n",
    "        #classification layer를 제외하고 pretrained weight를 생성된 모델로 로딩\n",
    "        util_keras.restore_ckpt(\n",
    "            model,\n",
    "            ckpt_path,\n",
    "            config.moving_average_decay,\n",
    "            exclude_layers=['class_net']\n",
    "        )\n",
    "    train.init_experimental(config)\n",
    "    return model\n",
    "\n",
    "model=get_efficientdet_model(config)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1402: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "WARNING:absl:float16 is not supported for CPU, use float32 instead\n",
      "WARNING:absl:Shape mismatch: class_net/class-predict/pointwise_kernel\n",
      "WARNING:absl:Shape mismatch: class_net/class-predict/bias\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b2 (Model)      multiple                  7267314   \n",
      "_________________________________________________________________\n",
      "resample_p6 (ResampleFeature multiple                  39984     \n",
      "_________________________________________________________________\n",
      "resample_p7 (ResampleFeature multiple                  0         \n",
      "_________________________________________________________________\n",
      "fpn_cells (FPNCells)         multiple                  678479    \n",
      "_________________________________________________________________\n",
      "class_net (ClassNet)         multiple                  51771     \n",
      "_________________________________________________________________\n",
      "box_net (BoxNet)             multiple                  52788     \n",
      "=================================================================\n",
      "Total params: 8,090,336\n",
      "Trainable params: 8,008,560\n",
      "Non-trainable params: 81,776\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(config.mode)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "traineval\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "sys.path.append('./automl/efficientdet')\n",
    "\n",
    "from keras import train\n",
    "import numpy as np\n",
    "\n",
    "config.batch_size=4\n",
    "train_steps_per_epoch=train_df.shape[0]\n",
    "test_steps_per_epoch=test_df.shape[0]\n",
    "print('train_steps_per_epoch : ',train_steps_per_epoch,\n",
    "    'test_steps_per_epoch : ', test_steps_per_epoch)\n",
    "\n",
    "test_dataset=get_dataset(False, config) if 'eval' in config.mode else None\n",
    "model.fit(\n",
    "    get_dataset(True, config),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    callbacks=train_lib.get_callbacks(config.as_dict(), test_dataset),\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_steps_per_epoch :  3123 test_steps_per_epoch :  761\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    ./automl/efficientdet/dataloader.py:443 None  *\n        anchor_labeler, params)\n    ./automl/efficientdet/dataloader.py:338 dataset_parser  **\n        num_positives) = anchor_labeler.label_anchors(boxes, classes)\n    ./automl/efficientdet/keras/anchors.py:238 label_anchors\n        anchor_box_list, gt_box_list, gt_labels)\n    ./automl/efficientdet/object_detection/target_assigner.py:144 assign\n        groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3120 ones\n        output = _constant_if_small(one, shape, dtype, name)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2804 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:6 prod\n        \n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3031 prod\n        keepdims=keepdims, initial=initial, where=where)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:855 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (parser/strided_slice_16:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3ecc2fda8493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     'test_steps_per_epoch : ', test_steps_per_epoch)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'eval'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m model.fit(\n\u001b[1;32m     14\u001b[0m     \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1c96e01859e3>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(is_training, config)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmax_instances_per_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_instances_per_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_CFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         config.as_dict())\n\u001b[0m",
      "\u001b[0;32m/mnt/sda1/KimHeeSu/munsu/Hanssem/automl/efficientdet/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, params, input_context, batch_size)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# pylint: enable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 447\u001b[0;31m         map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'drop_remainder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1810\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m           preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4245\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4246\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    ./automl/efficientdet/dataloader.py:443 None  *\n        anchor_labeler, params)\n    ./automl/efficientdet/dataloader.py:338 dataset_parser  **\n        num_positives) = anchor_labeler.label_anchors(boxes, classes)\n    ./automl/efficientdet/keras/anchors.py:238 label_anchors\n        anchor_box_list, gt_box_list, gt_labels)\n    ./automl/efficientdet/object_detection/target_assigner.py:144 assign\n        groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3120 ones\n        output = _constant_if_small(one, shape, dtype, name)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2804 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:6 prod\n        \n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3031 prod\n        keepdims=keepdims, initial=initial, where=where)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    /home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:855 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (parser/strided_slice_16:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('./automl/efficientdet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import hparams_config\n",
    "\n",
    "infer_config=hparams_config.get_efficientdet_config('efficientdet-d2')\n",
    "\n",
    "#config의 특정 학목을 update\n",
    "infer_config.model_name='efficientdet-d2'\n",
    "infer_config.model_dir=TRAIN_CFG.model_dir\n",
    "\n",
    "infer_config.num_classes=3\n",
    "infer_config.is_training_bn=False\n",
    "infer_config.nms_configs.score_thresh=0.4\n",
    "\n",
    "print(infer_config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "act_type: swish\n",
      "alpha: 0.25\n",
      "anchor_scale: 4.0\n",
      "apply_bn_for_resampling: true\n",
      "aspect_ratios:\n",
      "- 1.0\n",
      "- 2.0\n",
      "- 0.5\n",
      "autoaugment_policy: null\n",
      "backbone_config: null\n",
      "backbone_name: efficientnet-b2\n",
      "box_class_repeats: 3\n",
      "box_loss_weight: 50.0\n",
      "ckpt_var_scope: null\n",
      "clip_gradients_norm: 10.0\n",
      "conv_after_downsample: false\n",
      "conv_bn_act_pattern: false\n",
      "data_format: channels_last\n",
      "dataset_type: null\n",
      "delta: 0.1\n",
      "drop_remainder: true\n",
      "first_lr_drop_epoch: 200.0\n",
      "fpn_cell_repeats: 5\n",
      "fpn_config: null\n",
      "fpn_name: null\n",
      "fpn_num_filters: 112\n",
      "fpn_weight_method: null\n",
      "gamma: 1.5\n",
      "grad_checkpoint: false\n",
      "grid_mask: false\n",
      "heads:\n",
      "- object_detection\n",
      "image_size: 768\n",
      "img_summary_steps: null\n",
      "input_rand_hflip: true\n",
      "iou_loss_type: null\n",
      "iou_loss_weight: 1.0\n",
      "is_training_bn: false\n",
      "jitter_max: 2.0\n",
      "jitter_min: 0.1\n",
      "label_map: null\n",
      "label_smoothing: 0.0\n",
      "learning_rate: 0.08\n",
      "loss_scale: null\n",
      "lr_decay_method: cosine\n",
      "lr_warmup_epoch: 1.0\n",
      "lr_warmup_init: 0.008\n",
      "map_freq: 5\n",
      "max_instances_per_image: 100\n",
      "max_level: 7\n",
      "mean_rgb:\n",
      "- 123.675\n",
      "- 116.28\n",
      "- 103.53\n",
      "min_level: 3\n",
      "mixed_precision: false\n",
      "model_dir: /mnt/sda1/KimHeeSu/munsu/Hanssem/EfficientDet_Weight\n",
      "model_name: efficientdet-d2\n",
      "model_optimizations: {}\n",
      "momentum: 0.9\n",
      "moving_average_decay: 0.9998\n",
      "name: efficientdet-d2\n",
      "nms_configs:\n",
      "    iou_thresh: null\n",
      "    max_nms_inputs: 0\n",
      "    max_output_size: 100\n",
      "    method: gaussian\n",
      "    pyfunc: false\n",
      "    score_thresh: 0.4\n",
      "    sigma: null\n",
      "num_classes: 3\n",
      "num_epochs: 300\n",
      "num_scales: 3\n",
      "optimizer: sgd\n",
      "poly_lr_power: 0.9\n",
      "positives_momentum: null\n",
      "regenerate_source_id: false\n",
      "sample_image: null\n",
      "save_freq: epoch\n",
      "scale_range: false\n",
      "second_lr_drop_epoch: 250.0\n",
      "seg_num_classes: 3\n",
      "separable_conv: true\n",
      "skip_crowd_during_training: true\n",
      "skip_mismatch: true\n",
      "stddev_rgb:\n",
      "- 58.395\n",
      "- 57.120000000000005\n",
      "- 57.375\n",
      "strategy: null\n",
      "survival_prob: null\n",
      "target_size: null\n",
      "tflite_max_detections: 100\n",
      "use_keras_model: true\n",
      "var_freeze_expr: null\n",
      "verbose: 1\n",
      "weight_decay: 4.0e-05\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import inference\n",
    "from keras import efficientdet_keras\n",
    "import tensorflow as tfrecord\n",
    "\n",
    "model=efficientdet_keras.EfficientDetModel(config=infer_config)\n",
    "model.build((None, None, None, 3))\n",
    "print('### checkpoing name : ', tf.train.latest_checkpoint(infer_config.model_dir))\n",
    "model.load_weights(tf.train.latest_checkpoint(infer_config.model_dir))\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ResizeBilinear\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ResizeBilinear\n",
      "/home/plass-heesu/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1402: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting NonMaxSuppressionV5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting NonMaxSuppressionV5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### checkpoing name :  None\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6442c2180667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'### checkpoing name : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/munsutf2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   2815\u001b[0m           filepath.endswith('.hdf5'))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "class ExportModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    @tf.function\n",
    "    def f(self, imgs):\n",
    "        #model(imgs, training=False, post_mode='global')\n",
    "        return self.model(imgs, training=False, post_mode='global')\n",
    "\n",
    "export_model=ExportModel(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels_to_names={0:'수납장',1:'러그', 2:'침대'}\n",
    "#강의에서는 1:'car'였는데 그 이유가 모델이 1로 예측하면 출력은 car로 바꾸기 위해서"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_detected_img(export_model, img_array, is_print=True):\n",
    "    #automl efficient는 반환 bbox좌표값이 원본 이미지 좌표값으로 되어 있으므로\n",
    "    #별도의 scaling작업 필요 없음\n",
    "    #height=img_array.shape[0]\n",
    "    #width=img_array.shape[1]\n",
    "    #cv2의 rectangle()은 인자로 들어온 이미지 배열에 직접 사각형을 업데이트하므로\n",
    "    #그림 표현을 위한 별도의 이미지 배열 생성\n",
    "    draw_img=img_array.copy()\n",
    "\n",
    "    #bounding box의 테두리와 caption 글자색 지정\n",
    "    green_color=(0,255,0)\n",
    "    red_color=(255,0,0)\n",
    "\n",
    "    #cv2로 만들어진 numpy image array를 tensor로 변환\n",
    "    img_tensor=tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n",
    "    #img_tensor=tf.convert_to_tensor(img_array, dtype=tf.float32)[tf.newaxis, ...]\n",
    "\n",
    "    #efficientdet 모델을 다운로드 한 뒤 inference 수행\n",
    "    start_time=time.time()\n",
    "    #automl efficientdet 모델은 bbox, score, classes, num_detections를 각각 tensor로 반환\n",
    "    boxes, scores, classes, valid_len=export_model.f(img_tensor)\n",
    "    #Tensor값을 시각화를 위해 numpy로 변환\n",
    "    boxes=boxes.numpy()\n",
    "    scores=scores.numpy()\n",
    "    classes=classes.numpy()\n",
    "    valid_len=valid_len.numpy()\n",
    "\n",
    "    for i in range(valid_len[0]):\n",
    "        #detection score를 iteration시 마다 높은 순으로 추출하고 SCORE_THRESHOLD보다 낮으면 loop중단\n",
    "        score=scores[0,i]\n",
    "        box=boxes[0,i]\n",
    "\n",
    "        xmin=box[1]\n",
    "        ymin=box[0]\n",
    "        xmax=box[3]\n",
    "        ymax=box[2]\n",
    "\n",
    "        #class id 추출하고 class명으로 매핑\n",
    "        class_id=classes[0,i]\n",
    "        caption=\"{}: {:.4f}\".format(labels_to_names[class_id],score)\n",
    "        print(caption)\n",
    "        #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림\n",
    "        #위치 인자는 반드시 정수형\n",
    "        cv2.rectangle(draw_img, (int(xmin), int(ymin)), (int(xmax), int(ymax)),\n",
    "                    color=green_color, thickness=1)\n",
    "        cv2.putText(draw_img, caption, (int(xmin), int(ymin-5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.2, red_color, 1)\n",
    "        \n",
    "        if is_print:\n",
    "            print('Detection 수행시간 : ', round(time.time()-start_time, 2), \"초\")\n",
    "\n",
    "        return draw_img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_array=cv2.cvtColor(cv2.imread(png이미지 주소), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "draw_img=get_detected_img(export_model, img_array, is_print=True)\n",
    "plt.figure=(figsize=(16,16))\n",
    "plt.imshow(draw_img)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('munsutf2.4': conda)"
  },
  "interpreter": {
   "hash": "4773b342024c3ddfc468906d2951ce23c98dba948dfed0d75f23dbe62a25705b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}